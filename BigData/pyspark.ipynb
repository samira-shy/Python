{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73533c2d-00de-424e-92e7-52c33ba89df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68643e99-e56e-4b0f-9d75-6ec118dc2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960bbfbb-4441-49ac-bf94-b22dae250ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/06 21:41:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50039)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef6d712f-81bc-4941-90ae-b6d14af43ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.txt\n",
    "first line\n",
    "second line\n",
    "third line\n",
    "forth line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "263c9259-d7d4-4e00-adfc-53f12d53f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD using a textFile method\n",
    "textFile=sc.textFile('example.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595814f2-4fec-47cf-bb6c-87768703b3b5",
   "metadata": {},
   "source": [
    "## PySpark RDD Transformations & Actions\n",
    "\n",
    "* Transformations create a new RDD from an existing one.\n",
    "* Actions trigger execution and return results to the driver.\n",
    "* RDDs are immutable, meaning transformations create a new RDD.\n",
    "---\n",
    "\n",
    "# RDD Transformations\n",
    "\n",
    "| **Transformation**    | **Description** |\n",
    "|----------------------|----------------|\n",
    "| `map(func)`         | Applies `func` to each element and returns a new RDD |\n",
    "| `flatMap(func)`     | Similar to `map()`, but flattens nested lists |\n",
    "| `filter(func)`      | Filters elements based on `func` |\n",
    "| `groupByKey()`      | Groups data by key (used for key-value pairs) |\n",
    "| `reduceByKey(func)` | Applies `func` to reduce values for each key |\n",
    "| `sortByKey()`       | Sorts the RDD by key |\n",
    "| `join(rdd2)`        | Joins two RDDs based on keys |\n",
    "| `coalesce(n)`       | Reduces the number of partitions |\n",
    "| `distinct()`        | Removes duplicate values |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13332b-d963-4149-a132-f87702e0731b",
   "metadata": {},
   "source": [
    "## RDD Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbd8170f-462a-4408-b61a-ba5b7b8de327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "788cf6a5-1880-4d59-be71-5723f3232e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first line'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784f976-33d3-4fcf-b4e0-1999f39ecb24",
   "metadata": {},
   "source": [
    "## RDD Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c41a48-6e83-4ede-bb50-9c45d682e03c",
   "metadata": {},
   "source": [
    "### `filter()`: Filter specific values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdbe7d5f-d23c-41d2-919a-2075d7b4711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 transformation\n",
    "secfind=textFile.filter(lambda line:'line' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39115385-a27e-4c47-91dd-9d1e3887aea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first line', 'second line', 'third line', 'forth line']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2 action\n",
    "secfind.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cbf4dfa0-2642-4487-95e9-625d0d8dff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RDD: [('129', 450.0), ('131', 300.0)]\n"
     ]
    }
   ],
   "source": [
    "# Filter transactions greater than 200\n",
    "filtered_rdd = kv_rdd.filter(lambda x: x[1] > 200)\n",
    "print(\"Filtered RDD:\", filtered_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe0e3f-1553-49ed-b430-29938a73e284",
   "metadata": {},
   "source": [
    "### `groupByKey()`: Group data by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7440a6a6-6310-4f08-b0a0-ae8a5ebbdd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped RDD: [('121', [200.0]), ('122', [150.0]), ('131', [100.0, 300.0]), ('129', [450.0])]\n"
     ]
    }
   ],
   "source": [
    "# Group transactions by ServiceID\n",
    "grouped_rdd = kv_rdd.groupByKey().mapValues(list)\n",
    "print(\"Grouped RDD:\", grouped_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "660b36e3-f4ed-43d4-8930-034fe9500b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secfind.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a21b4d-975e-46b3-8362-28dfd6ab9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example2.txt\n",
    "first \n",
    "second line\n",
    "the third line\n",
    "then a fourth line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83101ed-820b-418a-8f57-0d1497c3614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3349572f-2f5d-4f3a-8c26-a1757cdd3bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/12 23:35:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c988bfe-df14-4dbf-bbae-f423707c84e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "example2.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show RDD\n",
    "sc.textFile('example2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaa27eb5-4693-49af-8af9-bb686c3ea502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a reference to this RDD\n",
    "text_rdd = sc.textFile('example2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efa46a6b-73f3-4c52-afa4-60c19276d7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['first'],\n",
       " ['second', 'line'],\n",
       " ['the', 'third', 'line'],\n",
       " ['then', 'a', 'fourth', 'line']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map a function (or lambda expression) to each line\n",
    "# Then collect the results.\n",
    "text_rdd.map(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19edfea6-2bbe-4b66-86fc-99e09add3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = text_rdd.map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6748bc76-fc58-4363-9041-36cc3e66802f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['first'],\n",
       " ['second', 'line'],\n",
       " ['third', 'line'],\n",
       " ['the', 'a', 'forth', 'line']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9b84bc-d54a-4e31-9253-d4e62efd2868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'second line', 'third line', 'the a forth line']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output is\n",
    "text_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244667e-2b9d-47ff-9eb2-c9da7ef6fc81",
   "metadata": {},
   "source": [
    "`flatMap()`: Flatten nested structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fbc157e6-cf82-4eb4-adee-7444ecf09adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlatMapped RDD: ['201', '10/13/2017', '100', 'NY', '131', '100.00', '204', '10/18/2017', '700', 'TX']\n"
     ]
    }
   ],
   "source": [
    "# Split rows into words\n",
    "words_rdd = rdd.flatMap(lambda line: line.split())\n",
    "print(\"FlatMapped RDD:\", words_rdd.take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0b1ff-233b-4077-bb30-8b4bf6d5e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split rows into words\n",
    "words_rdd = rdd.flatMap(lambda line: line.split())\n",
    "print(\"FlatMapped RDD:\", words_rdd.take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e94937d-6d30-4b64-b900-fff79d221bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'second',\n",
       " 'line',\n",
       " 'the',\n",
       " 'third',\n",
       " 'line',\n",
       " 'then',\n",
       " 'a',\n",
       " 'fourth',\n",
       " 'line']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect everything as a single flat map\n",
    "text_rdd.flatMap(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c2ef2-9a53-4170-b45a-a9b8d7e4767b",
   "metadata": {},
   "source": [
    "## RDDs and Key Value Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa513ed-9f48-4219-9ed5-c0cba1708aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing services.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile services.txt\n",
    "#EventId    Timestamp    Customer   State    ServiceID    Amount\n",
    "201       10/13/2017      100       NY       131          100.00\n",
    "204       10/18/2017      700       TX       129          450.00\n",
    "202       10/15/2017      203       CA       121          200.00\n",
    "206       10/19/2017      202       CA       131          500.00\n",
    "203       10/17/2017      101       NY       173          750.00\n",
    "205       10/19/2017      202       TX       121          200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3345472a-a3a3-4ac8-b9af-0ff3d62d73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "services = sc.textFile('services.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1e5cafb-33b1-4faa-a6e4-e0d259adcc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#EventId    Timestamp    Customer   State    ServiceID    Amount',\n",
       " '201       10/13/2017      100       NY       131          100.00']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 2 elements of RDD\n",
    "services.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654acc51-5c56-48e9-b33c-25f883de33eb",
   "metadata": {},
   "source": [
    "`map()`: Apply function to each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f00d1ae4-7e63-4264-b01f-5eb59be743c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped RDD: [('131', 100.0), ('129', 450.0), ('121', 200.0), ('131', 300.0), ('122', 150.0)]\n"
     ]
    }
   ],
   "source": [
    "# Extract ServiceID and Amount as key-value pairs\n",
    "kv_rdd = rdd.map(lambda line: (line.split()[4], float(line.split()[-1])))\n",
    "print(\"Mapped RDD:\", kv_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90a0681f-e8f2-4da8-81c5-f850dc5b39a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87cece27-6f18-4270-ab02-bf744f0dad04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n",
       " ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n",
       " ['204', '10/18/2017', '700', 'TX', '129', '450.00'],\n",
       " ['202', '10/15/2017', '203', 'CA', '121', '200.00'],\n",
       " ['206', '10/19/2017', '202', 'CA', '131', '500.00'],\n",
       " ['203', '10/17/2017', '101', 'NY', '173', '750.00'],\n",
       " ['205', '10/19/2017', '202', 'TX', '121', '200.00']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.map(lambda x: x.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45deffb-0687-47e6-abc9-073549094635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n",
       " ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n",
       " ['204', '10/18/2017', '700', 'TX', '129', '450.00']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.map(lambda x: x.split()).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1a49226-9d2a-4f3d-88ca-d21dc182ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data2.txt\n",
    "#ID    Name    Age    City    Score\n",
    "1      John    25     NY      85.5\n",
    "2      Jane    30     TX      92.0\n",
    "3      Alice   27     CA      88.0\n",
    "4      Bob     24     FL      79.5\n",
    "5      Eve     29     WA      95.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a42fcf2-c3b5-4513-9048-4f60dbf22b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EventId    Timestamp    Customer   State    ServiceID    Amount',\n",
       " '201       10/13/2017      100       NY       131          100.00',\n",
       " '204       10/18/2017      700       TX       129          450.00',\n",
       " '202       10/15/2017      203       CA       121          200.00',\n",
       " '206       10/19/2017      202       CA       131          500.00',\n",
       " '203       10/17/2017      101       NY       173          750.00',\n",
       " '205       10/19/2017      202       TX       121          200.00']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing hash tag\n",
    "services.map(lambda x: x[1:] if x[0]=='#' else x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "667ed3d4-eae6-4ecb-b9e6-704bd52bdc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n",
       " ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n",
       " ['204', '10/18/2017', '700', 'TX', '129', '450.00'],\n",
       " ['202', '10/15/2017', '203', 'CA', '121', '200.00'],\n",
       " ['206', '10/19/2017', '202', 'CA', '131', '500.00'],\n",
       " ['203', '10/17/2017', '101', 'NY', '173', '750.00'],\n",
       " ['205', '10/19/2017', '202', 'TX', '121', '200.00']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.map(lambda x: x[1:] if x[0]=='#' else x).map(lambda x: x.split()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e78daa-d795-4445-a310-c555076f261a",
   "metadata": {},
   "source": [
    "Here's your **Jupyter Notebook Markdown tutorial** for **RDD Actions** in PySpark. It includes a **pretext table** explaining each action and the corresponding **Python code** in proper markdown formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### **RDD Actions in PySpark**  \n",
    "\n",
    "#### **Action Description Table**\n",
    "| Action             | Description |\n",
    "|--------------------|------------|\n",
    "| `count()`         | Counts total rows |\n",
    "| `first()`         | Retrieves first row |\n",
    "| `collect()`       | Converts RDD to list |\n",
    "| `take(n)`        | Fetches first `n` rows |\n",
    "| `distinct()`      | Returns distinct rows |\n",
    "| `reduce()`        | Applies a function to aggregate values |\n",
    "| `max()`           | Finds the maximum value |\n",
    "| `min()`           | Finds the minimum value |\n",
    "| `countByKey()`    | Counts occurrences of each key |\n",
    "| `keys()`          | Retrieves all keys in an RDD |\n",
    "| `values()`        | Retrieves all values in an RDD |\n",
    "| `isEmpty()`       | Checks if RDD is empty |\n",
    "| `takeSample(False, n)` | Takes `n` random samples |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a64cbd82-b98b-46f3-a6f0-181a839b37c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile data.txt\n",
    "#EventId    Timestamp    Customer   State    ServiceID    Amount\n",
    "201       10/13/2017      100       NY       131          100.00\n",
    "204       10/18/2017      700       TX       129          450.00\n",
    "202       10/15/2017      203       CA       121          200.00\n",
    "205       10/20/2017      404       FL       131          300.00\n",
    "203       10/17/2017      305       TX       122          150.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dda6ebe0-5057-4af1-b1af-df56b03fe4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/apple/Documents/GitHub/Python/BigData\n",
      "['services.txt', '.DS_Store', 'example2.txt', 'lambda_expressions.ipynb', 'example.txt', '.ipynb_checkpoints', 'pyspark.ipynb', 'data.txt']\n"
     ]
    }
   ],
   "source": [
    "# To check the saved file, run:\n",
    "import os\n",
    "print(os.getcwd())  # Shows current working directory\n",
    "print(os.listdir())  # Lists files in the directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dfb2c393-1897-401d-9f0d-fb64e7995b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['201       10/13/2017      100       NY       131          100.00',\n",
       " '204       10/18/2017      700       TX       129          450.00',\n",
       " '202       10/15/2017      203       CA       121          200.00']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data.txt into an RDD (excluding header)\n",
    "rdd = sc.textFile(\"data.txt\").filter(lambda line: not line.startswith(\"#\"))\n",
    "\n",
    "# Display first few lines\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5f35a-137d-47da-8a30-7b64a6362eb3",
   "metadata": {},
   "source": [
    "## RDD Actions in PySpark\n",
    "- Actions are operations that **trigger computation and return values to the driver**.\n",
    "### **Basic Actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "969f7ca0-4d64-49f8-b981-9df8f77ce64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 5\n"
     ]
    }
   ],
   "source": [
    "# Count number of records\n",
    "print(\"Total rows:\", rdd.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "81a7a35e-a4d3-42af-acef-dd2fc9dc43ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: 201       10/13/2017      100       NY       131          100.00\n"
     ]
    }
   ],
   "source": [
    "# Show the first row\n",
    "print(\"First row:\", rdd.first())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5e32efa7-9f26-49ec-b83f-f3370d19907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records: ['201       10/13/2017      100       NY       131          100.00', '204       10/18/2017      700       TX       129          450.00', '202       10/15/2017      203       CA       121          200.00', '205       10/20/2017      404       FL       131          300.00', '203       10/17/2017      305       TX       122          150.00']\n"
     ]
    }
   ],
   "source": [
    "# Collect all records into a list\n",
    "print(\"All records:\", rdd.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a1a4323e-7126-446b-baf7-211e9bdca51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 rows: ['201       10/13/2017      100       NY       131          100.00', '204       10/18/2017      700       TX       129          450.00', '202       10/15/2017      203       CA       121          200.00']\n"
     ]
    }
   ],
   "source": [
    "# Take the first 3 rows\n",
    "print(\"First 3 rows:\", rdd.take(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b866cf94-d23b-4b9b-b653-4a58f48011be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct rows: ['204       10/18/2017      700       TX       129          450.00', '202       10/15/2017      203       CA       121          200.00', '205       10/20/2017      404       FL       131          300.00', '203       10/17/2017      305       TX       122          150.00', '201       10/13/2017      100       NY       131          100.00']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve distinct rows\n",
    "print(\"Distinct rows:\", rdd.distinct().collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690dd44-8a11-45e8-8f95-e694075cc147",
   "metadata": {},
   "source": [
    "### **Numeric Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e1c0a909-b16d-4941-9d0e-2635821e7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the \"Amount\" column (last column) and convert to float\n",
    "amount_rdd = rdd.map(lambda line: float(line.split()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1cc6d770-9b22-434e-8c84-bfc2586acc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Amount: 1200.0\n"
     ]
    }
   ],
   "source": [
    "# Find the total sum of Amount\n",
    "total_amount = amount_rdd.reduce(lambda x, y: x + y)\n",
    "print(\"Total Amount:\", total_amount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "10cb8d93-23a9-4329-87da-3f61fb5fb975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Amount: 450.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the maximum transaction amount\n",
    "max_amount = amount_rdd.max()\n",
    "print(\"Max Amount:\", max_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6214b66c-d897-428f-968e-2d34f2db1efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Amount: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Find the minimum transaction amount\n",
    "min_amount = amount_rdd.min()\n",
    "print(\"Min Amount:\", min_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9d1be10e-4be8-4356-9b27-0d18f8f65adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Amount: 240.0\n"
     ]
    }
   ],
   "source": [
    "# Compute average amount\n",
    "avg_amount = total_amount / amount_rdd.count()\n",
    "print(\"Average Amount:\", avg_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332b04e-6136-4a33-8c06-cda366ee8e3c",
   "metadata": {},
   "source": [
    "### **Some other Useful Actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ba334239-00b5-4719-acc4-9935307f9e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key-Value pairs: [('131', 100.0), ('129', 450.0)]\n"
     ]
    }
   ],
   "source": [
    "# Get the first 2 records as key-value pairs (ServiceID, Amount)\n",
    "kv_rdd = rdd.map(lambda line: (line.split()[4], float(line.split()[-1])))\n",
    "print(\"Key-Value pairs:\", kv_rdd.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e59e8173-1119-4f2b-81e8-266bdb61e7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count per ServiceID: {'131': 2, '129': 1, '121': 1, '122': 1}\n"
     ]
    }
   ],
   "source": [
    "# Count by key (ServiceID)\n",
    "service_count = kv_rdd.countByKey()\n",
    "print(\"Count per ServiceID:\", dict(service_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "16c34c27-004e-4a13-85c9-c254f80b3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ServiceIDs: ['131', '129', '121', '131', '122']\n"
     ]
    }
   ],
   "source": [
    "# Fetch all keys (ServiceIDs)\n",
    "print(\"All ServiceIDs:\", kv_rdd.keys().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5572f0df-ed28-4f48-b127-42e5d45e9808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Amounts: [100.0, 450.0, 200.0, 300.0, 150.0]\n"
     ]
    }
   ],
   "source": [
    "# Fetch all values (Amounts)\n",
    "print(\"All Amounts:\", kv_rdd.values().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0bc80e73-b135-44f2-8aca-01a669325cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is RDD empty? False\n"
     ]
    }
   ],
   "source": [
    "# Check if RDD is empty\n",
    "print(\"Is RDD empty?\", rdd.isEmpty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b940d267-19ff-432f-997a-a4fc72d35447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Records: ['201       10/13/2017      100       NY       131          100.00', '204       10/18/2017      700       TX       129          450.00']\n"
     ]
    }
   ],
   "source": [
    "# Take a sample (without replacement)\n",
    "print(\"Sample Records:\", rdd.takeSample(False, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef7d14-39bd-462b-85f4-f328ab837f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
