{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73533c2d-00de-424e-92e7-52c33ba89df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68643e99-e56e-4b0f-9d75-6ec118dc2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960bbfbb-4441-49ac-bf94-b22dae250ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/06 21:41:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50039)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef6d712f-81bc-4941-90ae-b6d14af43ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.txt\n",
    "first line\n",
    "second line\n",
    "third line\n",
    "forth line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "263c9259-d7d4-4e00-adfc-53f12d53f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD using a textFile method\n",
    "textFile=sc.textFile('example.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13332b-d963-4149-a132-f87702e0731b",
   "metadata": {},
   "source": [
    "## RDD Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbd8170f-462a-4408-b61a-ba5b7b8de327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "788cf6a5-1880-4d59-be71-5723f3232e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first line'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784f976-33d3-4fcf-b4e0-1999f39ecb24",
   "metadata": {},
   "source": [
    "## RDD Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdbe7d5f-d23c-41d2-919a-2075d7b4711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 transformation\n",
    "secfind=textFile.filter(lambda line:'line' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39115385-a27e-4c47-91dd-9d1e3887aea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first line', 'second line', 'third line', 'forth line']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2 action\n",
    "secfind.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "660b36e3-f4ed-43d4-8930-034fe9500b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secfind.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a21b4d-975e-46b3-8362-28dfd6ab9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example2.txt\n",
    "first \n",
    "second line\n",
    "the third line\n",
    "then a fourth line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83101ed-820b-418a-8f57-0d1497c3614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3349572f-2f5d-4f3a-8c26-a1757cdd3bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/09 23:35:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c988bfe-df14-4dbf-bbae-f423707c84e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "example2.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show RDD\n",
    "sc.textFile('example2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa27eb5-4693-49af-8af9-bb686c3ea502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a reference to this RDD\n",
    "text_rdd = sc.textFile('example2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efa46a6b-73f3-4c52-afa4-60c19276d7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['first'],\n",
       " ['second', 'line'],\n",
       " ['the', 'third', 'line'],\n",
       " ['then', 'a', 'fourth', 'line']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map a function (or lambda expression) to each line\n",
    "# Then collect the results.\n",
    "text_rdd.map(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e94937d-6d30-4b64-b900-fff79d221bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'second',\n",
       " 'line',\n",
       " 'the',\n",
       " 'third',\n",
       " 'line',\n",
       " 'then',\n",
       " 'a',\n",
       " 'fourth',\n",
       " 'line']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect everything as a single flat map\n",
    "text_rdd.flatMap(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c2ef2-9a53-4170-b45a-a9b8d7e4767b",
   "metadata": {},
   "source": [
    "## RDDs and Key Value Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa513ed-9f48-4219-9ed5-c0cba1708aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing services.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile services.txt\n",
    "#EventId    Timestamp    Customer   State    ServiceID    Amount\n",
    "201       10/13/2017      100       NY       131          100.00\n",
    "204       10/18/2017      700       TX       129          450.00\n",
    "202       10/15/2017      203       CA       121          200.00\n",
    "206       10/19/2017      202       CA       131          500.00\n",
    "203       10/17/2017      101       NY       173          750.00\n",
    "205       10/19/2017      202       TX       121          200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3345472a-a3a3-4ac8-b9af-0ff3d62d73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "services = sc.textFile('services.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1e5cafb-33b1-4faa-a6e4-e0d259adcc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#EventId    Timestamp    Customer   State    ServiceID    Amount',\n",
       " '201       10/13/2017      100       NY       131          100.00']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 2 elements of RDD\n",
    "services.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90a0681f-e8f2-4da8-81c5-f850dc5b39a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87cece27-6f18-4270-ab02-bf744f0dad04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n",
       " ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n",
       " ['204', '10/18/2017', '700', 'TX', '129', '450.00'],\n",
       " ['202', '10/15/2017', '203', 'CA', '121', '200.00'],\n",
       " ['206', '10/19/2017', '202', 'CA', '131', '500.00'],\n",
       " ['203', '10/17/2017', '101', 'NY', '173', '750.00'],\n",
       " ['205', '10/19/2017', '202', 'TX', '121', '200.00']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.map(lambda x: x.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45deffb-0687-47e6-abc9-073549094635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n",
       " ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n",
       " ['204', '10/18/2017', '700', 'TX', '129', '450.00']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.map(lambda x: x.split()).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a42fcf2-c3b5-4513-9048-4f60dbf22b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EventId    Timestamp    Customer   State    ServiceID    Amount',\n",
       " '201       10/13/2017      100       NY       131          100.00',\n",
       " '204       10/18/2017      700       TX       129          450.00',\n",
       " '202       10/15/2017      203       CA       121          200.00',\n",
       " '206       10/19/2017      202       CA       131          500.00',\n",
       " '203       10/17/2017      101       NY       173          750.00',\n",
       " '205       10/19/2017      202       TX       121          200.00']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing hash tag\n",
    "services.map(lambda x: x[1:] if x[0]=='#' else x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "667ed3d4-eae6-4ecb-b9e6-704bd52bdc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n",
       " ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n",
       " ['204', '10/18/2017', '700', 'TX', '129', '450.00'],\n",
       " ['202', '10/15/2017', '203', 'CA', '121', '200.00'],\n",
       " ['206', '10/19/2017', '202', 'CA', '131', '500.00'],\n",
       " ['203', '10/17/2017', '101', 'NY', '173', '750.00'],\n",
       " ['205', '10/19/2017', '202', 'TX', '121', '200.00']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.map(lambda x: x[1:] if x[0]=='#' else x).map(lambda x: x.split()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e78daa-d795-4445-a310-c555076f261a",
   "metadata": {},
   "source": [
    "Here's your **Jupyter Notebook Markdown tutorial** for **RDD Actions** in PySpark. It includes a **pretext table** explaining each action and the corresponding **Python code** in proper markdown formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### **RDD Actions in PySpark**  \n",
    "\n",
    "#### **Action Description Table**\n",
    "| Action             | Description |\n",
    "|--------------------|------------|\n",
    "| `count()`         | Counts total rows |\n",
    "| `first()`         | Retrieves first row |\n",
    "| `collect()`       | Converts RDD to list |\n",
    "| `take(n)`        | Fetches first `n` rows |\n",
    "| `distinct()`      | Returns distinct rows |\n",
    "| `reduce()`        | Applies a function to aggregate values |\n",
    "| `max()`           | Finds the maximum value |\n",
    "| `min()`           | Finds the minimum value |\n",
    "| `countByKey()`    | Counts occurrences of each key |\n",
    "| `keys()`          | Retrieves all keys in an RDD |\n",
    "| `values()`        | Retrieves all values in an RDD |\n",
    "| `isEmpty()`       | Checks if RDD is empty |\n",
    "| `takeSample(False, n)` | Takes `n` random samples |\n",
    "\n",
    "---\n",
    "\n",
    "### Load Data into an RDD**\n",
    "```python\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RDD Actions Tutorial\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create a text file (only needed if running locally)\n",
    "%%writefile data.txt\n",
    "#EventId    Timestamp    Customer   State    ServiceID    Amount\n",
    "201       10/13/2017      100       NY       131          100.00\n",
    "204       10/18/2017      700       TX       129          450.00\n",
    "202       10/15/2017      203       CA       121          200.00\n",
    "205       10/20/2017      404       FL       131          300.00\n",
    "203       10/17/2017      305       TX       122          150.00\n",
    "\n",
    "# Load data.txt into an RDD (excluding header)\n",
    "rdd = sc.textFile(\"data.txt\").filter(lambda line: not line.startswith(\"#\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Numeric Operations**\n",
    "```python\n",
    "# Extract the \"Amount\" column (last column) and convert to float\n",
    "amount_rdd = rdd.map(lambda line: float(line.split()[-1]))\n",
    "\n",
    "# Find the total sum of Amount\n",
    "total_amount = amount_rdd.reduce(lambda x, y: x + y)\n",
    "print(\"Total Amount:\", total_amount)\n",
    "\n",
    "# Find the maximum transaction amount\n",
    "max_amount = amount_rdd.max()\n",
    "print(\"Max Amount:\", max_amount)\n",
    "\n",
    "# Find the minimum transaction amount\n",
    "min_amount = amount_rdd.min()\n",
    "print(\"Min Amount:\", min_amount)\n",
    "\n",
    "# Compute average amount\n",
    "avg_amount = total_amount / amount_rdd.count()\n",
    "print(\"Average Amount:\", avg_amount)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Other Useful Actions**\n",
    "```python\n",
    "# Get the first 2 records as key-value pairs (ServiceID, Amount)\n",
    "kv_rdd = rdd.map(lambda line: (line.split()[4], float(line.split()[-1])))\n",
    "print(\"Key-Value pairs:\", kv_rdd.take(2))\n",
    "\n",
    "# Count by key (ServiceID)\n",
    "service_count = kv_rdd.countByKey()\n",
    "print(\"Count per ServiceID:\", dict(service_count))\n",
    "\n",
    "# Fetch all keys (ServiceIDs)\n",
    "print(\"All ServiceIDs:\", kv_rdd.keys().collect())\n",
    "\n",
    "# Fetch all values (Amounts)\n",
    "print(\"All Amounts:\", kv_rdd.values().collect())\n",
    "\n",
    "# Check if RDD is empty\n",
    "print(\"Is RDD empty?\", rdd.isEmpty())\n",
    "\n",
    "# Take a sample (without replacement)\n",
    "print(\"Sample Records:\", rdd.takeSample(False, 2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸŽ¯ Summary**\n",
    "- This tutorial covers **RDD Actions** in PySpark.\n",
    "- You learned **how to load data, manipulate RDDs, and apply various actions**.\n",
    "- Actions are operations that **trigger computation and return values to the driver**.\n",
    "\n",
    "Would you like me to add **RDD Transformations** (`map`, `filter`, `groupByKey`, etc.) as well? ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1a49226-9d2a-4f3d-88ca-d21dc182ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data.txt\n",
    "#ID    Name    Age    City    Score\n",
    "1      John    25     NY      85.5\n",
    "2      Jane    30     TX      92.0\n",
    "3      Alice   27     CA      88.0\n",
    "4      Bob     24     FL      79.5\n",
    "5      Eve     29     WA      95.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dda6ebe0-5057-4af1-b1af-df56b03fe4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/apple/Documents/GitHub/Python/BigData\n",
      "['services.txt', '.DS_Store', 'example2.txt', 'lambda_expressions.ipynb', 'example.txt', '.ipynb_checkpoints', 'pyspark.ipynb', 'data.txt']\n"
     ]
    }
   ],
   "source": [
    "# To check the saved file, run:\n",
    "import os\n",
    "print(os.getcwd())  # Shows current working directory\n",
    "print(os.listdir())  # Lists files in the directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2c393-1897-401d-9f0d-fb64e7995b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.txt into an RDD (excluding header)\n",
    "rdd = sc.textFile(\"data.txt\").filter(lambda line: not line.startswith(\"#\"))\n",
    "\n",
    "# Display first few lines\n",
    "rdd.take(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
